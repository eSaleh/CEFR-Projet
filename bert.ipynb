{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrNLVDemUliP"
      },
      "source": [
        "# Approche\n",
        "\n",
        "La technique d'apprentissage par transfert est utilisée pour résoudre le problème de classification de texte. Nous chargeons un modèle BERT préentraîné et ajustons ses poids.\n",
        "Avantages de l'ajustement fin\n",
        "\n",
        "  - Temps - Les poids du modèle BERT préentraîné encodent déjà beaucoup d’informations. Par conséquent, il faut beaucoup moins de temps pour affiner le modèle.\n",
        "\n",
        "  - Données - Étant donné que le modèle préentraîné a été entraîné sur un large corpus de texte, il donne de bons résultats même avec de petits ensembles de données.\n",
        "\n",
        "Nous n’entrons pas dans les détails de l’architecture de BERT. Voici un aperçu de la façon dont BERT est préentraîné et comment il peut être utilisé pour la classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGxdOr-WUliQ"
      },
      "source": [
        "## BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "Le modélisation du langage est une méthode courante de pré-entraînement sur du texte non annoté (apprentissage auto-supervisé). La plupart des modèles de langage sont entraînés en prédisant de manière itérative le mot suivant dans une séquence, de manière auto-régressive, sur d'immenses ensembles de données textuelles comme Wikipédia. Cette prédiction peut se faire de gauche à droite, de droite à gauche ou de manière bidirectionnelle.\n",
        "\n",
        "Il existe deux stratégies pour appliquer des représentations de langage pré-entraînées à des tâches en aval :\n",
        "\n",
        "* Approche basée sur les caractéristiques (Feature-based approach)\n",
        "* Approche d'affinage (Fine-tuning approach)\n",
        "\n",
        "L'approche basée sur les caractéristiques, comme ELMo, utilise des architectures spécifiques à la tâche qui incluent les représentations pré-entraînées en tant que caractéristiques supplémentaires.\n",
        "\n",
        "L'approche d'affinage, comme OpenAI GPT, introduit un minimum de paramètres spécifiques à la tâche et est entraînée sur la tâche cible en affinant tous les paramètres pré-entraînés.\n",
        "\n",
        "Le modèle BERT peut être utilisé pour les deux approches. Il reformule la tâche de pré-entraînement en modélisation du langage, qui consistait à prédire itérativement le mot suivant dans une séquence, pour au contraire intégrer un contexte bidirectionnel et prédire des tokens masqués intermédiaires dans la séquence. BERT a introduit une nouvelle tâche d'apprentissage auto-supervisé pour pré-entraîner les Transformers afin de les affiner pour différentes tâches. La principale différence entre BERT et les méthodes précédentes de pré-entraînement des modèles Transformers est l'utilisation du contexte bidirectionnel de la modélisation du langage. La plupart des modèles prédisent le mot suivant en se déplaçant soit de gauche à droite, soit de droite à gauche, alors que BERT apprend à prédire des tokens intermédiaires (grâce au mécanisme de MASK), d'où son nom de Bidirectional Encoder.\n",
        "\n",
        "BERT utilise deux tâches d'apprentissage :\n",
        "\n",
        "* Le modèle de langage masqué (Masked Language Model - MLM)\n",
        "* La prédiction de la phrase suivante (Next Sentence Prediction - NSP)\n",
        "\n",
        "BERT utilise trois types d'embeddings pour calculer les représentations d'entrée :\n",
        "\n",
        "* Token embeddings\n",
        "* Segment embeddings\n",
        "* Position embeddings\n",
        "\n",
        "Le Transformer de BERT conserve la longueur (ou la dimension) de l'entrée. La sortie finale prend ce vecteur et l'achemine vers différentes tâches, comme la classification dans ce cas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaRdu-leUliS"
      },
      "source": [
        "## Installation des librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEEpt_0IXPeh",
        "outputId": "dc6b294d-35a4-4694-8845-f92222654a82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reAU5qeLUliT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import preprocessing as pp\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
        "\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1X978ZPUliV",
        "outputId": "06c57b5d-03fd-421a-cf8d-8644e4ed35c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wP2BPHPiUliW"
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv(\"./datasets/X_y_train.csv\")\n",
        "df_cleaned = data_train[\"text\"].apply(lambda x : pp.remove_noise(x))\n",
        "y=data_train[\"level\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfArH0ZfUliX"
      },
      "source": [
        "# Data preprocessing\n",
        "\n",
        "En utilisant la librairie fournie par le challenge kaggle, preprocessing.py, nous appliquons un prétraitement pour enlever tout ce qui pourrait perturber la classification du modèle comme `\\n`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qocA683XUliY"
      },
      "source": [
        "### BERT Tokenizer\n",
        "\n",
        "De plus, nous appliquons l'algorithme de tokenization de BERT, qui se nomme WordPiece, pour transformer les textes en suites d'indice de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT2YE6kIUliZ",
        "outputId": "e69f3325-5001-412a-8b1a-884595c4d000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBnto5-mUlia",
        "outputId": "c2800def-d2ee-4ff4-b196-a915c7e44175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  Hi! Just some words about the product With the cost is in the average Market. The important differences are the features. a which could translatesuseful phrases intolanguages as the concurent But futhermore with out product we have a curruncy converter, headphone set, world radio, pedometer. Besides it is less heavy. An other concurrent has the advantage being smaller than ours but its more expensive and less of features. A big advantage, our product listens to your pronunciation and corrects inlanguages. I think we could logical saying is art. Best Regards. Bob\n",
            "Tokenized:  ['hi', '!', 'just', 'some', 'words', 'about', 'the', 'product', 'with', 'the', 'cost', 'is', 'in', 'the', 'average', 'market', '.', 'the', 'important', 'differences', 'are', 'the', 'features', '.', 'a', 'which', 'could', 'translates', '##use', '##ful', 'phrases', 'into', '##lang', '##ua', '##ges', 'as', 'the', 'con', '##cure', '##nt', 'but', 'fu', '##ther', '##more', 'with', 'out', 'product', 'we', 'have', 'a', 'cu', '##rr', '##un', '##cy', 'convert', '##er', ',', 'head', '##phone', 'set', ',', 'world', 'radio', ',', 'pe', '##dome', '##ter', '.', 'besides', 'it', 'is', 'less', 'heavy', '.', 'an', 'other', 'concurrent', 'has', 'the', 'advantage', 'being', 'smaller', 'than', 'ours', 'but', 'its', 'more', 'expensive', 'and', 'less', 'of', 'features', '.', 'a', 'big', 'advantage', ',', 'our', 'product', 'listen', '##s', 'to', 'your', 'pronunciation', 'and', 'correct', '##s', 'in', '##lang', '##ua', '##ges', '.', 'i', 'think', 'we', 'could', 'logical', 'saying', 'is', 'art', '.', 'best', 'regards', '.', 'bob']\n",
            "Token IDs:  [7632, 999, 2074, 2070, 2616, 2055, 1996, 4031, 2007, 1996, 3465, 2003, 1999, 1996, 2779, 3006, 1012, 1996, 2590, 5966, 2024, 1996, 2838, 1012, 1037, 2029, 2071, 16315, 8557, 3993, 15672, 2046, 25023, 6692, 8449, 2004, 1996, 9530, 23887, 3372, 2021, 11865, 12399, 5974, 2007, 2041, 4031, 2057, 2031, 1037, 12731, 12171, 4609, 5666, 10463, 2121, 1010, 2132, 9864, 2275, 1010, 2088, 2557, 1010, 21877, 26173, 3334, 1012, 4661, 2009, 2003, 2625, 3082, 1012, 2019, 2060, 16483, 2038, 1996, 5056, 2108, 3760, 2084, 14635, 2021, 2049, 2062, 6450, 1998, 2625, 1997, 2838, 1012, 1037, 2502, 5056, 1010, 2256, 4031, 4952, 2015, 2000, 2115, 15498, 1998, 6149, 2015, 1999, 25023, 6692, 8449, 1012, 1045, 2228, 2057, 2071, 11177, 3038, 2003, 2396, 1012, 2190, 12362, 1012, 3960]\n"
          ]
        }
      ],
      "source": [
        "# Un exemple\n",
        "\n",
        "print(' Original: ', df_cleaned[0])\n",
        "\n",
        "print('Tokenized: ', tokenizer.tokenize(df_cleaned[0]))\n",
        "\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df_cleaned[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIcNLJ4Ulia",
        "outputId": "35991943-352d-436f-cf6d-d7fd8ca36b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  509\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "for sent in df_cleaned:\n",
        "\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsObn8xkUlib",
        "outputId": "85e96849-e5b3-4d47-f1cd-001c1ce1f752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  Hi! Just some words about the product With the cost is in the average Market. The important differences are the features. a which could translatesuseful phrases intolanguages as the concurent But futhermore with out product we have a curruncy converter, headphone set, world radio, pedometer. Besides it is less heavy. An other concurrent has the advantage being smaller than ours but its more expensive and less of features. A big advantage, our product listens to your pronunciation and corrects inlanguages. I think we could logical saying is art. Best Regards. Bob\n",
            "Token IDs: tensor([  101,  7632,   999,  2074,  2070,  2616,  2055,  1996,  4031,  2007,\n",
            "         1996,  3465,  2003,  1999,  1996,  2779,  3006,  1012,  1996,  2590,\n",
            "         5966,  2024,  1996,  2838,  1012,  1037,  2029,  2071, 16315,  8557,\n",
            "         3993, 15672,  2046, 25023,  6692,  8449,  2004,  1996,  9530, 23887,\n",
            "         3372,  2021, 11865, 12399,  5974,  2007,  2041,  4031,  2057,  2031,\n",
            "         1037, 12731, 12171,  4609,  5666, 10463,  2121,  1010,  2132,  9864,\n",
            "         2275,  1010,  2088,  2557,  1010, 21877, 26173,  3334,  1012,  4661,\n",
            "         2009,  2003,  2625,  3082,  1012,  2019,  2060, 16483,  2038,  1996,\n",
            "         5056,  2108,  3760,  2084, 14635,  2021,  2049,  2062,  6450,  1998,\n",
            "         2625,  1997,  2838,  1012,  1037,  2502,  5056,  1010,  2256,  4031,\n",
            "         4952,  2015,  2000,  2115, 15498,  1998,  6149,  2015,  1999, 25023,\n",
            "         6692,  8449,  1012,  1045,  2228,  2057,  2071, 11177,  3038,  2003,\n",
            "         2396,  1012,  2190, 12362,  1012,  3960,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in df_cleaned:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len,      \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(y)\n",
        "\n",
        "print('Original: ', df_cleaned[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IGEUF22Ulic"
      },
      "source": [
        "#### Train-validation split\n",
        "80% des données sont réservées pour l'entraînement et le reste pour la validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uXztMxNUlic",
        "outputId": "8a6a621d-b6a9-4291-fbcd-b44fbf1f9f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10,696 training samples\n",
            "2,675 validation samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset)  - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AX5xNzP5Ulid"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7tr1GeUlid",
        "outputId": "213849e9-2d42-4fa9-cd5f-274a7363647d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Charger BertForSequenceClassification, le modèle BERT pré-entraîné avec une seule\n",
        "# couche linéaire de classification au-dessus.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 5,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHg2TFXeUlid",
        "outputId": "309f7087-8039-4dad-e979-b9fc0a6bb0d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43NOacmcUlie"
      },
      "source": [
        "# Affinage du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jjl_5ShGUlie"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a6wBGE6NUlie"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aFzOM3xhUlie"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O86tEQxUlif",
        "outputId": "aa3c7816-0244-41bf-89e8-81908e92669f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "335it [16:01,  2.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epcoh took: 0:16:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "335it [16:02,  2.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:16:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:34:49 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(1, epochs+1):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = output.loss\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    best_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        with torch.no_grad():\n",
        "            output= model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        loss = output.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        logits = output.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    if avg_val_accuracy > best_eval_accuracy:\n",
        "        torch.save(model, f'./bert_model_{epoch_i}.pt')\n",
        "        best_eval_accuracy = avg_val_accuracy\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnpxayCUlig"
      },
      "source": [
        "# Chargement de modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjNRCJNnUlig",
        "outputId": "251a719f-46bb-43ad-aded-8f7df615603b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-c2dd83bf0fa5>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('./bert_model_1.pt')\n"
          ]
        }
      ],
      "source": [
        "model = torch.load('./bert_model_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sHbABbrbUlih"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('./datasets/X_test.csv')\n",
        "df_test['text'] = df_test['text'].apply(lambda x:pp.remove_noise(x))\n",
        "test_df_cleaned = df_test['text'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bK2xy-_Ulih",
        "outputId": "fe604c33-bfd9-4815-fa2c-62915be19318"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_df_cleaned:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = max_len,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g1f82YgUlih"
      },
      "outputs": [],
      "source": [
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, \n",
        "            sampler = SequentialSampler(test_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5lw_6-5VUlih"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for batch in test_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        with torch.no_grad():\n",
        "            output= model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask)\n",
        "            logits = output.logits\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "            predictions.extend(list(pred_flat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOi-amp6Ulii"
      },
      "outputs": [],
      "source": [
        "df_output = pd.DataFrame(predictions,columns=[\"target\"])\n",
        "df_output_final = pd.concat((df_test['Id'],df_output),ignore_index=True)\n",
        "df_output_final.to_csv('submission_bert.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 869809,
          "sourceId": 17777,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30096,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Projet-CEFR",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
